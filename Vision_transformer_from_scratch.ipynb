{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "16U-HZKVwD7vSISrpQZgXEUTCSj1Fi-m-",
      "authorship_tag": "ABX9TyP+CUOVoJ9iYJCX278TM5rj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atiyehghm/DeepLearningSelfStudy/blob/main/Vision_transformer_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vision Transformer\n",
        "\n",
        "This notebook focuses on implementing VIT for Mnist dataset.\n",
        "\n",
        "[The main article on medium](https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c)"
      ],
      "metadata": {
        "id": "mx6zFuzeiG0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "og9rJ2tpigFB",
        "outputId": "3e09f75b-2112-459e-f99d-cb0d51267e87"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8Ef4FW1iAvC",
        "outputId": "75255b26-a756-48cc-e59e-5bf42f6986f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7815d0196ff0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets.mnist import MNIST\n",
        "\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tA7xE2dQA_dfzA0Bub5TVw.png)"
      ],
      "metadata": {
        "id": "9c7KnWCtjr4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Step1: patchify the image : (N, C, H, W) ---> (N, #of patches, patch dimensionality)\n",
        "## in our example which is a 28x28 image we convert it to 49 4x4 patches\n",
        "## Todo: Look for more efficient ways to do this\n",
        "\n",
        "def patchify(images, n_patches):\n",
        "  n,c, h, w = images.shape\n",
        "\n",
        "  assert h==w, \"Patchify only works for square images\"\n",
        "  patch_size = h//n_patches\n",
        "  patches = np.zeros((n, n_patches**2, (c*h*w)//(n_patches**2)))\n",
        "\n",
        "  for idx, image in enumerate(images):\n",
        "    for i in range(n_patches):\n",
        "      for j in range(n_patches):\n",
        "        patch = image[:, i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n",
        "        patches[idx, i*n_patches + j] = patch.flatten()\n",
        "\n",
        "  return patches"
      ],
      "metadata": {
        "id": "x6gQ2q1yjdFH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Testing the patchify\n",
        "x = torch.randn(7, 1, 28, 28) # Dummy images\n",
        "print(patchify(x, 7).shape) # torch.Size([7, 49, 16])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIhul-7PqlWA",
        "outputId": "5ea91574-08fe-45c1-dbdd-c84795d8fea8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7, 49, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "hidden_d: dimension of the output of linear mapping\n",
        "'''\n",
        "\n",
        "class VIT(nn.Module):\n",
        "  def __init__(self, chw=(1, 28, 28), n_patches=7, hidden_d=8):\n",
        "    super(VIT, self).__init__()\n",
        "\n",
        "    self.chw = chw\n",
        "    self.n_patches = n_patches\n",
        "    self.patch_size = (chw[1] // n_patches, chw[2] // n_patches)\n",
        "    self.hidden_d\n",
        "\n",
        "    assert chw[1]%n_patches, \"Input is not divisible by the number of patches\"\n",
        "    assert chw[2]%n_patches, \"Input is not divisible by the number of patches\"\n",
        "\n",
        "    ## Linear mapping\n",
        "    self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
        "    self.linear1 = nn.Linear(self.input_d, self.hidden_d)\n",
        "\n",
        "\n",
        "  def forward(self, images):\n",
        "    patches = patchify(images, self.n_patches)\n",
        "    tokens = self.linear1(patches)\n",
        "    return patches"
      ],
      "metadata": {
        "id": "6dscYtlMilK4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gm7dV7ihqOFb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}